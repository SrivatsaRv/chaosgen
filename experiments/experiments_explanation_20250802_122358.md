# ChaosGen AI-Generated Experiments Explanation

**Generated on**: 2025-08-02 12:23:58
**Total Experiments**: 3
**Generated by**: ChaosGen AI-Powered Chaos Engineering Platform

---

## Overview

This document explains the chaos experiments generated by ChaosGen's AI analysis of your Kubernetes infrastructure. Each experiment is designed to test specific failure scenarios and validate your system's resilience.

## Infrastructure Analysis

ChaosGen discovered the following services in your cluster:
- **Frontend Services**: User-facing applications (nginx, web interfaces)
- **Backend Services**: Business logic and API services (Flask, Node.js, etc.)
- **Database Services**: Data persistence layers (PostgreSQL, Redis, etc.)
- **Infrastructure Services**: Monitoring, logging, and platform services

## Generated Experiments


### Experiment 1: frontend-pod-delete

**Target**: app=frontend (deployment) in namespace `demo`
**Chaos Type**: pod-delete
**Risk Level**: MEDIUM

#### What This Experiment Does


This experiment randomly kills pods from the app=frontend deployment to test:
- **Pod restart resilience**: How quickly new pods start up
- **Load balancer behavior**: How traffic is redistributed
- **Service discovery**: How clients find healthy pods
- **Data consistency**: Whether state is preserved during restarts

This simulates real-world scenarios like:
- Node failures
- OOM kills
- Manual pod deletions
- Rolling updates gone wrong

#### Parameters

- **TOTAL_CHAOS_DURATION**: 30
- **CHAOS_INTERVAL**: 10
- **FORCE**: false

#### Expected Impact


- **Immediate**: app=frontend pods will be terminated
- **Recovery**: New pods should start within 30-60 seconds
- **User Impact**: Brief service interruption (5-30 seconds)
- **Monitoring**: Watch for increased error rates and latency
- **Success Criteria**: Service returns to normal within 2 minutes

#### Safety Measures

- **Abort Conditions**: Experiment will stop if system health degrades
- **Duration Control**: Limited execution time to prevent extended outages
- **Resource Limits**: Controlled chaos intensity to maintain system stability

#### Monitoring Points

- Application response times
- Error rates and availability
- Resource utilization (CPU, memory, network)
- Service dependency health
- Database connection pools

---

### Experiment 2: api-cpu-hog

**Target**: app=api (deployment) in namespace `demo`
**Chaos Type**: pod-cpu-hog
**Risk Level**: LOW

#### What This Experiment Does


This experiment injects CPU stress into app=api pods to test:
- **Resource limits**: Whether CPU limits are properly enforced
- **Performance degradation**: How the app behaves under load
- **Scheduler behavior**: How Kubernetes handles resource pressure
- **Monitoring alerts**: Whether CPU spikes trigger proper alerts

This simulates:
- High CPU usage from bugs
- Resource exhaustion attacks
- Poorly optimized code paths
- Background processing spikes

#### Parameters

- **TOTAL_CHAOS_DURATION**: 60
- **CPU_CORES**: 1
- **FORCE**: false

#### Expected Impact


- **Immediate**: app=api pods will experience high CPU usage
- **Performance**: Response times may increase by 50-200%
- **User Impact**: Slower application responses
- **Monitoring**: Watch CPU metrics and response times
- **Success Criteria**: App remains functional, performance recovers after chaos

#### Safety Measures

- **Abort Conditions**: Experiment will stop if system health degrades
- **Duration Control**: Limited execution time to prevent extended outages
- **Resource Limits**: Controlled chaos intensity to maintain system stability

#### Monitoring Points

- Application response times
- Error rates and availability
- Resource utilization (CPU, memory, network)
- Service dependency health
- Database connection pools

---

### Experiment 3: mongo-pod-delete

**Target**: app=mongo (statefulset) in namespace `litmus`
**Chaos Type**: pod-delete
**Risk Level**: MEDIUM

#### What This Experiment Does


This experiment randomly kills pods from the app=mongo deployment to test:
- **Pod restart resilience**: How quickly new pods start up
- **Load balancer behavior**: How traffic is redistributed
- **Service discovery**: How clients find healthy pods
- **Data consistency**: Whether state is preserved during restarts

This simulates real-world scenarios like:
- Node failures
- OOM kills
- Manual pod deletions
- Rolling updates gone wrong

#### Parameters

- **TOTAL_CHAOS_DURATION**: 45
- **CHAOS_INTERVAL**: 15
- **FORCE**: false

#### Expected Impact


- **Immediate**: app=mongo pods will be terminated
- **Recovery**: New pods should start within 30-60 seconds
- **User Impact**: Brief service interruption (5-30 seconds)
- **Monitoring**: Watch for increased error rates and latency
- **Success Criteria**: Service returns to normal within 2 minutes

#### Safety Measures

- **Abort Conditions**: Experiment will stop if system health degrades
- **Duration Control**: Limited execution time to prevent extended outages
- **Resource Limits**: Controlled chaos intensity to maintain system stability

#### Monitoring Points

- Application response times
- Error rates and availability
- Resource utilization (CPU, memory, network)
- Service dependency health
- Database connection pools

---

## How to Use These Experiments

### 1. Review and Understand
- Read each experiment explanation above
- Understand what failure scenario it tests
- Verify the target services are correct

### 2. Safety Considerations
- All experiments include abort thresholds
- Monitor your system during execution
- Have rollback procedures ready

### 3. Execution
```bash
# Apply all experiments
kubectl apply -f experiments/

# Apply specific experiment
kubectl apply -f experiments/experiment-name.yaml

# Monitor execution
kubectl get chaosengine -n <namespace>
kubectl get chaosresult -n <namespace>
```

### 4. Analysis
- Monitor application metrics during chaos
- Check system recovery after chaos
- Document findings and improvements

## Safety Features

Each experiment includes:
- **Abort Thresholds**: Automatic stopping if metrics exceed limits
- **Resource Validation**: Checks before execution
- **Gradual Intensity**: Controlled chaos injection
- **Recovery Monitoring**: Validates system healing

## Next Steps

1. **Review**: Understand each experiment's purpose
2. **Test**: Run in non-production environment first
3. **Monitor**: Watch metrics and logs during execution
4. **Analyze**: Document resilience gaps and improvements
5. **Iterate**: Generate new experiments based on findings

---

*Generated by ChaosGen - AI-Powered Chaos Engineering*
