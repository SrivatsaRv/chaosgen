#!/usr/bin/env python3
"""
Chaos Advisor Agent Demo

Demonstrates the complete workflow of the chaos engineering agent:
1. Fetch infrastructure inventory
2. Generate experiment suggestions
3. Execute an experiment
4. Monitor execution
5. Generate RCA report
"""

import os
import sys
import json
import time
from pathlib import Path

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent))

from tools.inventory_fetch import InventoryFetchTool
from tools.experiment_designer import ExperimentDesigner
from tools.executor_adapter import ExecutorAdapter
from tools.run_monitor import RunMonitor
from tools.post_run_narrator import PostRunNarrator


def print_step(step_num: int, title: str, description: str = ""):
    """Print a formatted step header"""
    print(f"\n{'='*60}")
    print(f"STEP {step_num}: {title}")
    print(f"{'='*60}")
    if description:
        print(description)
    print()


def demo_inventory_fetch():
    """Demo step 1: Fetch infrastructure inventory"""
    print_step(1, "Infrastructure Discovery", "Fetching Kubernetes service inventory...")
    
    try:
        # Initialize inventory tool
        inventory_tool = InventoryFetchTool()
        
        # Fetch inventory
        topology = inventory_tool.run(Path("stack.yaml"))
        
        # Display results
        print(f"âœ… Discovered {len(topology.get('services', []))} services")
        print(f"ğŸ“Š Cluster: {topology.get('metadata', {}).get('cluster', {}).get('kubernetes_version', 'Unknown')}")
        
        # Show services by type
        services = topology.get('services', [])
        by_type = {}
        for service in services:
            service_type = service.get('type', 'unknown')
            if service_type not in by_type:
                by_type[service_type] = []
            by_type[service_type].append(service)
        
        for service_type, type_services in by_type.items():
            print(f"   ğŸ“¦ {service_type.upper()}: {len(type_services)} services")
            for service in type_services[:3]:  # Show first 3
                print(f"      â€¢ {service['name']} ({service['namespace']})")
            if len(type_services) > 3:
                print(f"      ... and {len(type_services) - 3} more")
        
        return topology
        
    except Exception as e:
        print(f"âŒ Failed to fetch inventory: {e}")
        print("Using mock topology for demo...")
        
        # Return mock topology for demo
        return {
            "services": [
                {
                    "id": "sock-shop/frontend",
                    "name": "frontend",
                    "namespace": "sock-shop",
                    "type": "deployment",
                    "env": "k8s",
                    "critical": True
                },
                {
                    "id": "sock-shop/redis",
                    "name": "redis",
                    "namespace": "sock-shop",
                    "type": "statefulset",
                    "env": "k8s",
                    "critical": True
                },
                {
                    "id": "sock-shop/cart",
                    "name": "cart",
                    "namespace": "sock-shop",
                    "type": "deployment",
                    "env": "k8s",
                    "critical": True
                }
            ],
            "metadata": {
                "cluster": {
                    "kubernetes_version": "v1.25.0",
                    "provider": "demo"
                }
            }
        }


def demo_experiment_design(topology):
    """Demo step 2: Generate experiment suggestions"""
    print_step(2, "AI-Powered Experiment Design", "Generating chaos experiment suggestions...")
    
    try:
        # Initialize experiment designer
        designer = ExperimentDesigner()
        
        # Generate experiments
        experiments = designer.design(topology, count=3)
        
        # Display results
        print(f"âœ… Generated {len(experiments)} experiment suggestions")
        print()
        
        for i, experiment in enumerate(experiments, 1):
            print(f"ğŸ“‹ Experiment {i}: {experiment['title']}")
            print(f"   Description: {experiment['description']}")
            print(f"   Action: {experiment['action']}")
            print(f"   Target: {experiment['target_selector'].get('namespace')}/{experiment['target_selector'].get('label_selector')}")
            print(f"   Risk Level: {experiment['risk_level']}")
            print(f"   Duration: {experiment['parameters'].get('duration', 'Unknown')}")
            print()
        
        # Save experiments
        experiments_dir = Path("experiments")
        experiments_dir.mkdir(exist_ok=True)
        
        for i, experiment in enumerate(experiments):
            filename = f"{experiment['title'].lower().replace(' ', '_')}.json"
            filepath = experiments_dir / filename
            
            with open(filepath, 'w') as f:
                json.dump(experiment, f, indent=2)
            
            print(f"ğŸ’¾ Saved: {filepath}")
        
        return experiments
        
    except Exception as e:
        print(f"âŒ Failed to generate experiments: {e}")
        print("Using mock experiments for demo...")
        
        # Return mock experiments
        return [
            {
                "title": "Redis Pod Kill",
                "description": "Test Redis failover mechanism",
                "action": "pod-kill",
                "target_selector": {
                    "namespace": "sock-shop",
                    "label_selector": "app=redis"
                },
                "parameters": {
                    "duration": "60s",
                    "intensity": 0.5
                },
                "risk_level": "medium"
            }
        ]


def demo_experiment_execution(experiments):
    """Demo step 3: Execute an experiment"""
    print_step(3, "Experiment Execution", "Executing chaos experiment...")
    
    if not experiments:
        print("âŒ No experiments to execute")
        return None
    
    # Use the first experiment
    experiment = experiments[0]
    print(f"ğŸš€ Executing: {experiment['title']}")
    print(f"   Action: {experiment['action']}")
    print(f"   Target: {experiment['target_selector']}")
    print(f"   Duration: {experiment['parameters'].get('duration')}")
    print()
    
    try:
        # Initialize executor
        adapter = ExecutorAdapter()
        
        # Execute experiment (dry run for demo)
        print("ğŸ” Running in dry-run mode...")
        run_id = adapter.apply(experiment, dry_run=True)
        
        print(f"âœ… Experiment validated successfully")
        print(f"ğŸ†” Run ID: {run_id}")
        
        return run_id, experiment
        
    except Exception as e:
        print(f"âŒ Failed to execute experiment: {e}")
        print("Using mock execution for demo...")
        
        run_id = f"demo-run-{int(time.time())}"
        print(f"ğŸ†” Mock Run ID: {run_id}")
        
        return run_id, experiment


def demo_monitoring(run_id, experiment):
    """Demo step 4: Monitor experiment execution"""
    print_step(4, "Real-time Monitoring", "Monitoring experiment execution and metrics...")
    
    print(f"ğŸ“Š Monitoring run: {run_id}")
    print(f"ğŸ¯ Target: {experiment['title']}")
    print()
    
    try:
        # Initialize monitor
        monitor = RunMonitor()
        
        # Simulate monitoring (for demo)
        print("â±ï¸  Simulating 60-second experiment execution...")
        
        for i in range(6):  # 6 updates over 60 seconds
            time.sleep(1)  # Simulate time passing
            
            # Simulate metrics
            error_rate = 0.01 + (i * 0.005)  # Gradually increasing
            latency = 0.100 + (i * 0.020)    # Gradually increasing
            
            print(f"   ğŸ“ˆ Update {i+1}/6: Error Rate: {error_rate:.3f}, Latency: {latency:.3f}s")
            
            # Check abort conditions
            abort_threshold = experiment.get('abort_threshold', {})
            if abort_threshold.get('metric') == 'error_rate' and error_rate > abort_threshold.get('value', 0.05):
                print(f"   ğŸ›‘ Abort condition met: Error rate {error_rate:.3f} > {abort_threshold.get('value', 0.05)}")
                break
        
        print("âœ… Monitoring completed")
        
        # Return mock monitoring data
        return {
            "status": "completed",
            "duration_seconds": 60,
            "final_metrics": {
                "error_rate": 0.035,
                "latency_p95": 0.220,
                "cpu_usage": 0.45,
                "memory_usage": 0.60
            },
            "impact_analysis": {
                "error_rate_impact_percent": 15.0,
                "latency_impact_percent": 25.0
            }
        }
        
    except Exception as e:
        print(f"âŒ Failed to monitor experiment: {e}")
        return None


def demo_rca_generation(run_id, experiment, monitoring_data):
    """Demo step 5: Generate RCA report"""
    print_step(5, "Root Cause Analysis", "Generating comprehensive RCA report...")
    
    print(f"ğŸ“ Generating RCA for run: {run_id}")
    print(f"ğŸ¯ Experiment: {experiment['title']}")
    print()
    
    try:
        # Initialize narrator
        narrator = PostRunNarrator()
        
        # Generate report
        report_path = narrator.generate_report(run_id, experiment, monitoring_data)
        
        print(f"âœ… RCA report generated: {report_path}")
        print()
        
        # Show report summary
        print("ğŸ“Š Report Summary:")
        print(f"   ğŸ“„ Full Report: {report_path}")
        print(f"   ğŸ“„ Latest Report: reports/latest.md")
        print()
        
        # Show key metrics
        if monitoring_data:
            metrics = monitoring_data.get('final_metrics', {})
            impact = monitoring_data.get('impact_analysis', {})
            
            print("ğŸ“ˆ Key Metrics:")
            for metric, value in metrics.items():
                if isinstance(value, float):
                    print(f"   â€¢ {metric}: {value:.3f}")
                else:
                    print(f"   â€¢ {metric}: {value}")
            
            print()
            print("ğŸ“Š Impact Analysis:")
            for impact_type, value in impact.items():
                print(f"   â€¢ {impact_type}: {value:.1f}%")
        
        return report_path
        
    except Exception as e:
        print(f"âŒ Failed to generate RCA: {e}")
        return None


def main():
    """Main demo function"""
    print("ğŸ¤– Chaos Advisor Agent - Complete Workflow Demo")
    print("=" * 60)
    print()
    print("This demo shows the complete chaos engineering workflow:")
    print("1. Infrastructure Discovery")
    print("2. AI-Powered Experiment Design")
    print("3. Experiment Execution")
    print("4. Real-time Monitoring")
    print("5. Root Cause Analysis")
    print()
    
    # Check environment
    if not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸  Warning: OPENAI_API_KEY not set. Some features may not work.")
        print("   Set it with: export OPENAI_API_KEY='your-api-key'")
        print()
    
    # Run demo steps
    try:
        # Step 1: Inventory fetch
        topology = demo_inventory_fetch()
        
        # Step 2: Experiment design
        experiments = demo_experiment_design(topology)
        
        # Step 3: Experiment execution
        execution_result = demo_experiment_execution(experiments)
        if execution_result:
            run_id, experiment = execution_result
        else:
            print("âŒ Demo failed at execution step")
            return
        
        # Step 4: Monitoring
        monitoring_data = demo_monitoring(run_id, experiment)
        
        # Step 5: RCA generation
        report_path = demo_rca_generation(run_id, experiment, monitoring_data)
        
        # Final summary
        print_step(6, "Demo Complete", "Chaos engineering workflow demonstration finished!")
        print("ğŸ‰ All steps completed successfully!")
        print()
        print("ğŸ“‹ What we accomplished:")
        print("   âœ… Discovered infrastructure services")
        print("   âœ… Generated AI-powered experiment suggestions")
        print("   âœ… Executed chaos experiment (dry-run)")
        print("   âœ… Monitored execution and metrics")
        print("   âœ… Generated comprehensive RCA report")
        print()
        print("ğŸš€ Next steps:")
        print("   1. Configure your real infrastructure in stack.yaml")
        print("   2. Set up LitmusChaos in your Kubernetes cluster")
        print("   3. Run real experiments: python craterctl.py suggest")
        print("   4. Execute experiments: python craterctl.py run experiments/<file>.json")
        print()
        print("ğŸ“š For more information, see README.md")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Demo interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Demo failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 